<!DOCTYPE html>
<html lang="en">
<head>

        <title>La traduction du réglement européen sur l’IA</title>
        <meta charset="utf-8" />
        <link href="https://anr-matos.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MaTOS - MAchine Translation for Open Science Full Atom Feed" />
        <link href="https://anr-matos.github.io/feeds/miscellaneous.atom.xml" type="application/atom+xml" rel="alternate" title="MaTOS - MAchine Translation for Open Science Categories Atom Feed" />


        <!-- Mobile viewport optimized: j.mp/bplateviewport -->
        <meta name="viewport" content="width=device-width,initial-scale=1, maximum-scale=1">

        <link rel="stylesheet" type="text/css" href="../../../../theme/gumby.css" />
        <link rel="stylesheet" type="text/css" href="../../../../theme/style.css" />
        <link rel="stylesheet" type="text/css" href="../../../../theme/pygment.css" />

        <script src="../../../../theme/js/libs/modernizr-2.6.2.min.js"></script>




</head>

<body id="index" class="home">


    <div class="container">

        <div class="row">

          <header id="banner" class="body">
                  <h1><a href="../../../../">MaTOS - MAchine Translation for Open Science <strong>Science in more than one language</strong></a></h1>
          </header><!-- /#banner -->

            <div id="navigation" class="navbar row">
              <a href="#" gumby-trigger="#navigation &gt; ul" class="toggle"><i class="icon-menu"></i></a>
             
              <ul class="columns">
                <li><a href="../../../../">Home</a></li>

                <li><a href="http://anr-matos.github.io/en/pages/presentation"> Presentation</a></li>
                <li><a href="http://anr-matos.github.io/en/pages/background"> Background</a></li>
                <li><a href="http://anr-matos.github.io/en/pages/publications"> Publications</a></li>
                <li><a href="http://anr-matos.github.io/en/pages/resources"> Resources</a></li>
                <li><a href="http://anr-matos.github.io/en/pages/news"> News</a></li>
                <li><a href="http://anr-matos.github.io/en/archives"> Blog</a></li>
                <li><a href="http://anr-matos.github.io/en/pages/partners"> Partners</a></li>
                <li><a href="mailto:contact@anr-matos.fr"> Contact</a></li>
	    <li style="background-color: white; padding: 5px;">&nbsp</li>
		<li><a href="https://anr-matos.github.io/">Français</a></li>
		<li class="active"><a href="https://anr-matos.github.io/en/">English</a></li>
              </ul>
            </div>

<section id="content" class="body">

   <div class="row">
        <div class="eleven columns">


            <header>
              <h2 class="entry-title">
                <a href="../../../.././posts/2023/oct/01/la-traduction-du-reglement-europeen-sur-lia.html" rel="bookmark"
                   title="Permalink to La traduction du réglement européen sur l’IA">La traduction du réglement européen sur l&#8217;<span class="caps">IA</span></a></h2>
           Translations:
<a href="../../../.././posts/2023/mai/20/la-traduction-du-reglement-europeen-sur-lia.html" hreflang="fr">fr</a>

            </header>
            <footer class="post-info">
              <abbr class="published" title="2023-10-01T00:00:00+02:00">
                Dim 01 oct 2023
              </abbr>
              <address class="vcard author">By 
                <a class="url fn" href="../../../../"> Matos</a>
              </address>
            </footer><!-- /.post-info -->
            <div class="entry-content">
              <p>La science se fait en anglais, mais le droit européen, lui se fait dans toutes les langues de l&#8217;Union. Parfois, il arrive que le droit parle de science, de science en train de se faire, comme dans le cas des projets de réglementation des systèmes d&#8217;Intelligence Artificielle, qui sont en cours de débat au sein des institutions européennes, et qui ont du être amendés récemment du fait de l&#8217;émergence des giga modèles de langue. Cette contribution illustre l&#8217;importance de disposer d&#8217;une terminologie scientifique à jour pour parler clairement, dans des termes compréhensibles par le plus grand nombre, des technologies en train d&#8217;émerger et des enjeux qu&#8217;elles&nbsp;posent.</p>
<hr>
<p>La <a href="https://eur-lex.europa.eu/legal-content/FR/ALL/?uri=CELEX%3A52021PC0206">Législation européenne sur l&#8217;intelligence artificielle</a> en cours d&#8217;élaboration au sein des institutions de l&#8217;<span class="caps">UE</span> a connu une avancée récente avec le vote du parlement européen le 14 juin 2023 d&#8217;un texte révisé, qui doit maintenant être négocié avec le conseil et les états&nbsp;membres.</p>
<p>Le texte adopté par les parlementaires est disponible sur le site du Parlement&nbsp;Européen: </p>
<ul>
<li><a href="https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html">Version anglaise</a>, consultée le 19 juin&nbsp;2023</li>
<li><a href="https://www.europarl.europa.eu/doceo/document/A-9-2023-0188_FR.html">Version française</a>, consultée le 19 juin&nbsp;2023</li>
</ul>
<p>Nous commentons simplement quelques-uns des amendements les plus récents, liés aux développements des giga modèles de langue. Ces modèles posent un problème à l&#8217;équilibre général du texte qui jusqu&#8217;à présent avait soigneusement évité de définir précisément les technologies, en se focalisant sur les finalités de celles-ci ; selon leur niveau de danger (du plus au moins risqué), la réglementation prévoyait d&#8217;augmenter le niveau d&#8217;encadrement, pouvant aller jusqu&#8217;à l&#8217;interdiction totale des technologies les plus dangeureuses (par exemple la notation&nbsp;sociale). </p>
<p>Commençons par l&#8217;amendement 34.
| Anglais | Français |
|&#8212;&#8212;&#8212;&#8212;-| &#8212;&#8212;&#8212;&#8212;-|
| (12c)  The developers of free and open-source <span class="caps">AI</span> components should not be mandated under this Regulation to comply with requirements targeting the <span class="caps">AI</span> value chain and, in particular, not towards the provider that has used that free and open-source <span class="caps">AI</span> component. Developers of free and open-source <span class="caps">AI</span> components should however be encouraged to implement widely adopted documentation practices, such as <strong>model and data cards</strong>, as a way to accelerate information sharing along the <span class="caps">AI</span> value chain, allowing the promotion of trustworthy <span class="caps">AI</span> systems in the Union. | (12 quater) Les développeurs de composants d’<span class="caps">IA</span> libres et ouverts ne devraient pas être tenus, en vertu du présent règlement, de se conformer aux exigences ciblant la chaîne de valeur de l’<span class="caps">IA</span> et, en particulier, aux exigences vis-à-vis du fournisseur qui a utilisé ce composant d’<span class="caps">IA</span> libre et ouvert. Les développeurs de composants d’<span class="caps">IA</span> libres et ouverts devraient toutefois être encouragés à mettre en œuvre des pratiques documentaires largement adoptées, telles que <strong>des modèles et des cartes de données</strong>, afin d’accélérer le partage d’informations tout au long de la chaîne de valeur de l’<span class="caps">IA</span>, ce qui permettrait de promouvoir des systèmes d’<span class="caps">IA</span> dignes de confiance dans l’Union.|&nbsp;|&#8212;&#8212;&#8212;&#8212;-|&#8212;&#8212;&#8212;&#8212;&#8212;|</p>
<p><strong>Commentaires:</strong>
La version anglaise utilise (de manière factorisée et donc ambigüe) les termes &#8216;model cards&#8217; et &#8216;data cards&#8217; qui font référence à des pratiques de documentation de modèles et de jeux de données en cours de formalisation et qui ont été introduits par M. Mitchell et ses co-auteurs dans: <a href="https://arxiv.org/abs/1810.03993">Model Cards for Model Reporting</a> et par M. Pushkarna et al dans <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3533231">Data cards: Purposeful and transparent dataset documentation for responsible <span class="caps">AI</span></a> . La version française traduit &#8216;modèles et cartes de données&#8217;, un double contresens qui conduit à perdre la notion de &#8216;carte (d&#8217;identité) du modèle&#8217;. Une traduction plus juste serait &#8216;carte d&#8217;identité du modèle&#8217; et &#8216;carte d&#8217;identité des&nbsp;données&#8217;.</p>
<p>Passons à l&#8217;amendement (60e)
| Anglais | Français |
|&#8212;&#8212;&#8212;&#8212;-| &#8212;&#8212;&#8212;&#8212;-|
| (60e) <strong>Foundation models</strong> are a recent development, in which <span class="caps">AI</span> models are developed from algorithms designed to optimize for generality and versatility of output. Those models are often <strong>trained</strong> on a broad range of data sources and large amounts of data to accomplish a wide range of <strong>downstream tasks</strong>, including some for which they were not specifically developed and trained. The foundation model can be unimodal or multimodal, trained through various methods such as supervised learning or reinforced learning. <span class="caps">AI</span> systems with specific intended purpose or general purpose <span class="caps">AI</span> systems can be an implementation of a foundation model, which means that each foundation model can be reused in countless downstream <span class="caps">AI</span> or general purpose <span class="caps">AI</span> systems. These models hold growing importance to many <strong>downstream application</strong>s and systems. | (60 sexies) Les <strong>systèmes d’<span class="caps">IA</span> à finalité générale</strong> sont une évolution récente, dans le cadre de laquelle des modèles d’<span class="caps">IA</span> sont développés à partir d’algorithmes conçus pour optimiser la généralité et la polyvalence de la production. Ces modèles sont souvent <strong>formés</strong> sur un large éventail de sources de données et de grandes quantités de données pour accomplir un large éventail de <strong>tâches en aval</strong>, y compris certaines pour lesquelles ils n’ont pas été spécifiquement développés et <strong>formés</strong>. Le système d’<span class="caps">IA</span> à finalité générale peut être unimodal ou multimodal, formé au moyen de diverses méthodes telles que l’apprentissage supervisé ou l’apprentissage renforcé. Les systèmes d’<span class="caps">IA</span> ayant une destination spécifique ou les systèmes d’<span class="caps">IA</span> à usage général peuvent être la mise en œuvre d’un système d’<span class="caps">IA</span> à finalité générale, ce qui signifie que chaque système d’<span class="caps">IA</span> à finalité générale peut être réutilisé dans d’innombrables <strong>systèmes d’<span class="caps">IA</span> en aval</strong> ou à usage général. Ces modèles revêtent une importance croissante pour de nombreuses <strong>applications et systèmes en aval</strong>.|
|&#8212;&#8212;&#8212;&#8212;-|&nbsp;&#8212;&#8212;&#8212;&#8212;-|</p>
<p><strong>Commentaires:</strong>
&#8216;<a href="https://www.arxiv.org/abs/2108.07258">Foundation models</a>&#8216; est une proposition récente du Human-Centered Artificial Intelligence de Stanford qui ne fait pas complètement consensus mais qui désigne spécifiquement les modèles profonds préentrainés (voir par exemple <a href="https://en.wikipedia.org/wiki/Foundation_models">Wikipédia</a>); la version française traduit de manière assez systématique &#8216;systèmes d&#8217;<span class="caps">IA</span> à finalité générale&#8217;, qui est toutefois bien plus général que &#8216;modèle de fondation&#8217; et pourrait aussi bien désigner des modèles probabilistes, symboliques ou hybrides. Cette traduction a le mérite de bien mettre en évidence le nouveau problème posé par ces modèles (ils n&#8217;ont pas de finalité précise), mais la confusion fâcheurse entre &#8216;modèles&#8217; et &#8216;systèmes&#8217; rend la fin de ce paragraphe complètement incompréhensible car le texte anglais explique qu&#8217;un modèle de fondation peut servir à implanter des systèmes soit à destination spécifique, soit généraux; comme les deux concepts sont confondus dans le texte français, la dernière phrase du texte français semble introduire une différence entre &#8216;systèmes à finalité générale&#8217; et &#8216;systèmes à usage&nbsp;général&#8217;.</p>
<p><span class="quo">&#8216;</span>foundation(al) model&#8217; pourrait être mieux traduit par &#8216;modèle de fondation&#8217; comme dans le récent <a href="https://anr.fr/fr/france-2030/france2030/call/ia-cluster-poles-de-recherche-et-de-formation-de-rang-mondial-en-intelligence-artificielle-app/">appel de l&#8217;<span class="caps">ANR</span> sur les (sic) &#8216;<span class="caps">IA</span>-clusters&#8217; (pôles de recherche et formation en <span class="caps">IA</span>)</a>; c&#8217;est également le terme retenu par le Comité National Pilote d&#8217;Ethique du Numérique (<span class="caps">CNPEN</span>) dans son récent avis (7) sur les <a href="https://www.ccne-ethique.fr/sites/default/files/2023-07/CNPEN-Avis7.pdf">Intelligences Artificielles Genératives</a>.</p>
<p>Dans sa présentation des &#8216;modèles de fondation&#8217;, la version anglaise discute de leur apprentissage en termes généraux. L&#8217;anglais utilise &#8216;supervised learning&#8217; et &#8216;reinforced learning&#8217;. Le second est probablement une erreur pour &#8216;reinforcement learning&#8217;, que le français traduit par &#8216;apprentissage renforcé&#8217; au lieu d&#8217;utiliser la terminologie établie &#8216;apprentissage par renforcement&#8217;. Plus fondamentalement, il y a peut-être ici un contresens dans la compréhension des &#8216;foundation models&#8217;, que l&#8217;article original de Stanford (section 1.1) caractérise précisément par leur incomplétude et par le fait qu&#8217;ils sont entraînés sans&nbsp;supervision.</p>
<p><span class="quo">&#8216;</span>tâche en aval&#8217; est attesté, mais c&#8217;est une traduction littérale de &#8216;downstream task&#8217;, qui est typiquement utilisée pour décrire des applications qui impliquent une succession de traitement; &#8216;tâches finales&#8217; serait sans doute meilleur pour marquer le fait qu&#8217;il s&#8217;agit d&#8217;applications&nbsp;finalisées.</p>
<p>Enfin, on note que &#8216;trained&#8217; est plusieurs fois traduit par &#8216;formé(s)&#8217; (&#8216;souvent formés&#8217;, &#8216;développés et formés&#8217;), à la place de &#8216;entraîné(s)&#8217; qui est la traduction&nbsp;correcte.</p>
<hr>
<table>
<thead>
<tr>
<th>Anglais</th>
<th>Français</th>
</tr>
</thead>
<tbody>
<tr>
<td>(60f)  In the case of <strong>foundation models provided as a service such as through <span class="caps">API</span> access</strong>, the cooperation with downstream providers should extend throughout the time during which that service is provided and supported, in order to enable appropriate risk mitigation, unless the provider of the foundation model transfers the training model as well as extensive and appropriate information on the datasets and the development process of the system or restricts the service, such as the <span class="caps">API</span> access, in such a way that the downstream provider is able to fully comply with this Regulation without further support from the original provider of the foundation model.</td>
<td>(60 septies) Dans le cas de <strong>systèmes d’<span class="caps">IA</span> à finalité générale fournis sous la forme d’un service comme par accès <span class="caps">API</span></strong>, la coopération avec les fournisseurs en aval devrait s’étendre pendant toute la durée de fourniture et de soutien de ce service, afin de permettre une atténuation appropriée des risques, à moins que le fournisseur du système d’<span class="caps">IA</span> à finalité générale ne transfère le système d’<span class="caps">IA</span> à finalité générale ainsi que des informations détaillées et appropriées sur les ensembles de données et le processus de développement du système ou restreint le service, comme l’accès <span class="caps">API</span>, de manière à ce que le fournisseur en aval soit en mesure de se conformer pleinement au présent règlement sans le soutien supplémentaire du fournisseur initial du système d’<span class="caps">IA</span> à finalité générale.</td>
</tr>
<tr>
<td>&#8212;&#8212;&#8212;&#8212;-</td>
<td>&#8212;&#8212;&#8212;&#8212;-</td>
</tr>
</tbody>
</table>
<p><span class="quo">&#8216;</span>comme par accès <span class="caps">API</span>&#8217; (such as through <span class="caps">API</span>) n&#8217;est pas très heureux - en écho à &#8216;software as a service&#8217; / logiciel en tant que service, on pourrait suggérer &#8216;modèles de fondation en tant que services fournis au travers d&#8217;<span class="caps">API</span>&#8217;. On note qu&#8217;il existe un terme recommandé pour <span class="caps">API</span>: <a href="https://www.culture.fr/franceterme/terme/INFO960?domaine=0&amp;q=API">Interface de Programmation&nbsp;d&#8217;Application</a></p>
<p><span class="quo">&#8216;</span>downstream providers&#8217; - fournisseurs en aval, voir commentaire&nbsp;supra.</p>
<p>Poursuivons la lecture avec l&#8217;amendement&nbsp;suivant.</p>
<table>
<thead>
<tr>
<th>Anglais</th>
<th>Français</th>
</tr>
</thead>
<tbody>
<tr>
<td>60g)  In light of the nature and complexity of the value chain for <span class="caps">AI</span> system, it is essential to clarify the role of actors contributing to the development of <span class="caps">AI</span> systems. There is significant uncertainty as to the way foundation models will evolve, both in terms of typology of models and in terms of self-governance. Therefore, it is essential to clarify the legal situation of providers of foundation models. Combined with their complexity and unexpected impact, the downstream <span class="caps">AI</span> provider’s lack of control over the foundation model’s development and the consequent power imbalance and in order to ensure a fair sharing of responsibilities along the <span class="caps">AI</span> value chain, such models should be subject to proportionate and more specific requirements and obligations under this Regulation, namely foundation models should assess and mitigate possible risks and harms through appropriate design, testing and analysis, should implement data governance measures, including assessment of biases, and should comply with technical design requirements to ensure appropriate levels of performance, predictability, interpretability, corrigibility, safety and cybersecurity and should comply with environmental standards. These obligations should be accompanied by standards. Also, foundation models should have information obligations and prepare all necessary technical documentation for potential downstream providers to be able to comply with their obligations under this Regulation. Generative foundation models should ensure transparency about the fact the content is generated by an <span class="caps">AI</span> system, not by humans. These specific requirements and obligations do not amount to considering foundation models as high risk <span class="caps">AI</span> systems, but should guarantee that the objectives of this Regulation to ensure a high level of protection of fundamental rights, health and safety, environment, demoacracy and rule of law are achieved. <strong>Pre-trained</strong> models developed for a narrower, less general, more limited set of applications that cannot be adapted for a wide range of tasks such as <strong>simple multi-purpose <span class="caps">AI</span> systems</strong> should not be considered foundation models for the purposes of this Regulation, because of their greater <strong>interpretability</strong> which makes their behaviour less unpredictable.</td>
<td>(60 octies) Compte tenu de la nature et de la complexité de la chaîne de valeur des systèmes d’<span class="caps">IA</span>, il est essentiel de faire la lumière sur le rôle des acteurs qui contribuent au développement des système d’<span class="caps">IA</span>. Il existe une grande incertitude quant à la manière dont les systèmes d’<span class="caps">IA</span> à finalité générale évolueront, tant en ce qui concerne la typologie des modèles que l’autogouvernance. Il est donc essentiel de clarifier la situation juridique des fournisseurs de systèmes d’<span class="caps">IA</span> à finalité générale. Combinés à leur complexité et à leur incidence inattendue, le manque de contrôle exercé par le fournisseur d’<span class="caps">IA</span> en aval sur le développement du système d’<span class="caps">IA</span> à finalité générale et sur le déséquilibre de pouvoir qui en résulte, et afin de garantir un partage équitable des responsabilités tout au long de la chaîne de valeur de l’<span class="caps">IA</span>, ces modèles devraient être soumis à des exigences et obligations proportionnées et plus spécifiques au titre du présent règlement, à savoir que les systèmes d’<span class="caps">IA</span> à finalité générale devraient évaluer et atténuer les risques et les préjudices éventuels au moyen d’une conception, d’essais et d’analyses appropriés, mettre en œuvre des mesures de gouvernance des données, y compris l’évaluation des biais, et respecter les exigences en matière de conception technique afin de garantir des niveaux appropriés de performance, de prévisibilité, d’interprétation, de prévisibilité, de sécurité et de cybersécurité, et devraient être conformes aux normes environnementales. Ces obligations devraient s’accompagner de normes. En outre, les systèmes d’<span class="caps">IA</span> à finalité générale devraient être soumis à des obligations d’information et préparer toute la documentation technique nécessaire pour permettre aux fournisseurs en aval potentiels de se conformer aux obligations qui leur incombent en vertu du présent règlement. Les systèmes d’<span class="caps">IA</span> à finalité générale génératifs devraient garantir la transparence quant au fait que le contenu est généré par un système d’<span class="caps">IA</span>, et non par un humain. Ces exigences et obligations spécifiques n’équivalent pas à considérer les systèmes d’<span class="caps">IA</span> à finalité générale comme des systèmes d’<span class="caps">IA</span> à haut risque, mais devraient garantir que les objectifs du présent règlement visant à garantir un niveau élevé de protection des droits fondamentaux, de la santé et de la sécurité, de l’environnement, de la démocratie et de l’état de droit sont atteints. Les <strong>modèles préformés</strong> élaborés pour un ensemble d’applications plus restreint, moins général et plus limité, qui ne peuvent être adaptés à un large éventail de tâches, telles que les simples systèmes d’<span class="caps">IA</span> polyvalents, ne devraient pas être considérés comme des systèmes d’<span class="caps">IA</span> à finalité générale aux fins du présent règlement, en raison de leur plus grande <strong>capacité d’interprétation</strong>, ce qui rend leur comportement moins imprévisible.</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Commentaire:</strong> En plus de la reprise de confusions qui existent dans les sections précédentes et la reprise de &#8216;préformés&#8217; au lieu de &#8216;préentraînés&#8217;, la version française comporte une erreur de grammaire dans la dernière phrase &#8216;modèles préformés (&#8230;), telles (=&gt; tels) que les simples systèmes polyvalents (&#8230;) ne devraient pas être&nbsp;considérés.&#8217;</p>
<p>Un contresens s&#8217;est glissé dans la dernière phrase: l&#8217;anglais parle &#8216;d&#8217;interprétabilité&#8217; (&#8216;greater interpretability&#8217;), sans doute de manière un peu hasardeuse car il sera difficile de dresser une frontière claire entre ce qui est considéré comme modèle de fondation et ce qui ne l&#8217;est pas; le français parle &#8216;d&#8217;interprétation&#8217;, comme si les modèles dont il est question pouvaient produire des interprétations de leurs sorties. Le lien qui est tissé ici entre &#8216;interprétabilité&#8217; et &#8216;imprédictabilité&#8217; n&#8217;est pas très&nbsp;clair.</p>
<p>Le texte introduit enfin (peut-être) un nouveau concept &#8216;simple multi-purpose <span class="caps">AI</span> systems&#8217; (donc un système qui n&#8217;a pas qu&#8217;une seule finalité, mais plusieurs, contrairement à un système &#8216;à finalité&nbsp;générale&#8217;).</p>
<hr>
<table>
<thead>
<tr>
<th>Anglais</th>
<th>Français</th>
</tr>
</thead>
<tbody>
<tr>
<td>60h)  Given the nature of foundation models, expertise in conformity assessment is lacking and third-party auditing methods are still under development. The sector itself is therefore developing new ways to assess <strong>fundamental models</strong> that fulfil in part the objective of auditing (such as model evaluation, <strong>red-teaming</strong> or <strong>machine learning verification and validation techniques</strong>). Those internal assessments for foundation models should be should be broadly applicable (e.g. independent of distribution channels, modality, development methods), to address risks specific to such models taking into account industry state-of-the-art practices and focus on developing sufficient technical understanding and control over the model, the management of reasonably foreseeable risks, and extensive analysis and testing of the model through appropriate measures, such as by the involvement of independent evaluators. As foundation models are a new and fast-evolving development in the field of artificial intelligence, it is appropriate for the Commission and the <span class="caps">AI</span> Office to monitor and periodically asses the legislative and governance framework of such models and in particular of generative <span class="caps">AI</span> systems based on such models, which raise significant questions related to the generation of content in breach of Union law, copyright rules, and potential misuse. It should be clarified that this Regulation should be without prejudice to Union law on copyright and related rights, including Directives 2001/29/<span class="caps">EC</span>, 2004/48/<span class="caps">ECR</span> and (<span class="caps">EU</span>) 2019/790 of the European Parliament and of the Council.</td>
<td>(60 nonies) Compte tenu de la nature des systèmes d’<span class="caps">IA</span> à finalité générale, l’expertise en matière d’évaluation de la conformité fait défaut et des méthodes d’audit par des tiers sont toujours en cours d’élaboration. Le secteur lui-même développe donc de nouveaux moyens d’évaluer les systèmes d’<span class="caps">IA</span> à finalité générale qui répondent en partie à l’objectif de l’audit (tels que l’évaluation des modèles, la <strong>méthode de l’équipe rouge («red teaming»)</strong> ou les <strong>techniques de vérification et de validation de l’apprentissage automatique</strong>). Ces évaluations internes des systèmes d’<span class="caps">IA</span> à finalité générale devraient être largement applicables (par exemple, indépendamment des canaux de distribution, des modalités, des méthodes de développement), afin de traiter les risques propres à ces modèles en tenant compte des pratiques les plus récentes du secteur et de mettre l’accent sur le développement d’une compréhension technique et d’un contrôle suffisants du modèle, sur la gestion des risques raisonnablement prévisibles, ainsi que sur une analyse et des essais approfondis du modèle au moyen de mesures appropriées, par exemple par la participation d’évaluateurs indépendants. Étant donné que les systèmes d’<span class="caps">IA</span> à finalité générale constituent une évolution nouvelle et rapide dans le domaine de l’intelligence artificielle, il convient que la Commission et le Bureau de l’<span class="caps">IA</span> surveillent et évaluent périodiquement le cadre législatif et de gouvernance de ces modèles et, en particulier, des systèmes d’<span class="caps">IA</span> génératifs fondés sur de tels modèles, qui soulèvent des questions importantes liées à la production de contenus en violation du droit de l’Union, aux règles en matière de droit d’auteur et à d’éventuels abus. Il convient de préciser que le présent règlement devrait être sans préjudice du droit de l’Union sur le droit d’auteur et les droits voisins, y compris les directives 2001/29/<span class="caps">CE</span>, 2004/48/<span class="caps">CE</span> et (<span class="caps">UE</span>) 2019/790 du Parlement européen et du Conseil.</td>
</tr>
</tbody>
</table>
<p><strong>Commentaire:</strong> 
Une erreur ou confusion peut-être dans le texte anglais qui parle maintenant de &#8216;fundamental model&#8217;, pour lequel le français conserve &#8216;système d&#8217;<span class="caps">IA</span> à finalité&nbsp;générale&#8217;. </p>
<p><span class="quo">&#8216;</span>red-teaming&#8217; est traduit de manière littérale par &#8216;l&#8217;équipe rouge&#8217;, mais les traducteurs ont pris soin de rappeler le terme anglais. En sécurité informatique &#8220;red-teaming&#8221; renvoie aux stratégies de mises à l&#8217;épreuve de systèmes qui emploient des méthodes adverses, en essayant de simuler ce que ferait un agresseur de l&#8217;<a href="https://fr.wikipedia.org/wiki/%C3%89quipe_rouge">équipe rouge</a> (la &#8220;red-team&#8221; désignant souvent en langage militaire l&#8217;ennemi, et la &#8220;blue-team&#8221; les alliés). Il est douteux que cette interprétation soit très bien connue - une traduction possiblement plus transparente serait dans le contexte &#8216;(..) l&#8217;étude de leur comportement face à des attaques malveillantes&#8217; etc. Le texte continue par un contresens, &#8216;techniques de vérification et validation de l&#8217;apprentissage automatique&#8217; quand l&#8217;anglais dit  &#8216;techniques d&#8217;apprentissage automatique pour la vérification et la&nbsp;validation&#8217;.</p>
            </div><!-- /.entry-content -->


        </div><!-- /.eleven.columns -->

<div class="three columns">

  </ul>
  





</div> </div><!-- /.row -->


</section>

       </div><!-- /.row -->
    </div><!-- /.container -->

    <footer id="credits" class="row">
      
    <img src="../../../../static/images/Logo-banner.jpeg" alt="Partenaires" height="80"/>
    
    <div class="seven columns left-center">
      
      <address id="about" class="vcard body">
      Powered by <a href="http://getpelican.com/">Pelican</a>,
      which takes great advantage of <a href="http://python.org">Python</a>.
      <br />
      Based on the <a target="_blank" href="http://gumbyframework.com">Gumby Framework</a>
      </address>
      
    </div>

    <div class="seven columns">
    <div class="row">
      <ul class="socbtns">
	
	<li><div class="btn primary"><a href="https://anr-matos.github.io" target="_blank">Github</a></div></li>
	
	
	
	
      </ul>
    </div>
    </div>
    </footer>

    </div>


  <script src="../../../../theme/js/libs/jquery-1.9.1.min.js"></script>
  <script src="../../../../theme/js/libs/gumby.min.js"></script>
  <script src="../../../../theme/js/plugins.js"></script>
</body>
</html>