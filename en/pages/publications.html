<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Publications | ùïÑaTOS
</title>
  <link rel="canonical" href="https://anr-matos.github.io/en/pages/publications.html">

    <link rel="apple-touch-icon" href="https://anr-matos.github.io/en/static/images/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="https://anr-matos.github.io/en/static/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://anr-matos.github.io/en/static/images//favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="https://anr-matos.github.io/en/static/images/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" media="screen" href="https://fontlibrary.org//face/linux-libertine" type="text/css"/>
  <link rel="stylesheet" href="https://anr-matos.github.io/en/./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://anr-matos.github.io/en/./theme/css/fontawesome.min.css">
  <link rel="stylesheet" href="https://anr-matos.github.io/en/theme/css/pygments/default.min.css">
<link rel="stylesheet" href="https://anr-matos.github.io/en/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://anr-matos.github.io/feeds/all.atom.xml">
  
  <meta name="description" content="Here you can find the publications produced in the context of the project, which can also be found on the HAL website.">


</head>

<body>
<header class="header">


<nav class="navbar navbar-expand-md navbar-light shadow-sm">
<a class="navbar-brand" href="#"></a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav m-0 ml-auto p-0">
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/index.html" data-target="#about">
		    <span> Home</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/pages/background.html" data-target="#about">
		    <span> Background</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/pages/publications.html" data-target="#about">
		    <span> Publications</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/pages/resources.html" data-target="#about">
		    <span> Resources</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/pages/news.html" data-target="#about">
		    <span> News</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/archives.html" data-target="#about">
		    <span> Blog</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://anr-matos.github.io/en/pages/partners.html" data-target="#about">
		    <span> Partners</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="mailto:contact@anr-matos.fr" data-target="#about">
		    <span> Contact</span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://bsky.app/profile/anr-matos.bsky.social" data-target="#about">
		    <span><i class="fab fa-bluesky" aria-hidden="true"></i></span>
		  </a>
		</li>
		<li class="nav-item m-0">
		  <a class="nav-link m-0" href="https://github.com/ANR-Matos" data-target="#about">
		    <span><i class="fab fa-github" aria-hidden="true"></i></span>
		  </a>
		</li>
	      </ul>
  </div>
  </nav>

    <div class="container">
<div class="row">
  
    <div class="col-sm-2 float-right">
      <!-- <a href="https://anr-matos.github.io/en/"> -->
        <img class="img-fluid rounded float-right" src=https://anr-matos.github.io/en/static/images/matos-logo.png width=160 height=160 alt="ùïÑaTOS">
      <!-- </a> -->
    </div>
  <div class="col-sm-10">
    <h1 class="title">
      <!-- <a href="https://anr-matos.github.io/en/"> -->
        ùïÑaTOS
      <!-- </a> -->
    </h1>
      <p class="text-muted subtitle">MAchine Translation for Open Science</p>


	
  </div>
</div>    </div>
  </header>

    <div class="main">

    <div class="container">
  <article class="article">
<div class="content">
  
  <div class="float-right">
 
          <a href="https://anr-matos.github.io//pages/publications.html">[Version fran√ßaise üá´üá∑]</a>
   </div><br>

  	<p><p>Here you can find the publications produced in the context of the project, which can also be found on the <a href="https://anr.hal.science/search/index/?q=*&amp;anrProjectReference_s=ANR-22-CE23-0033"><span class="caps">HAL</span> website</a>.</p></p>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://aclanthology.org/2025.wmt-1.13/
">Self-Retrieval from Distant Contexts for Document-Level Machine Translation</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Ziqian Peng, Rachel Bawden et Fran√ßois Yvon. Self-Retrieval from Distant Contexts for Document-Level Machine Translation. Proceedings of the Tenth Conference on Machine Translation, pages 220‚Äì240, Suzhou, China. Association for Computational Linguistics.
</p>
    <small class="text-muted">Document-level machine translation is a challenging task, as it requires modeling both short-range and long-range
dependencies to maintain the coherence and cohesion of the generated translation. However, these dependencies are
sparse, and most context-augmented translation systems resort to two equally unsatisfactory options: either to include
maximally long contexts, hoping that the useful dependencies are not lost in the noise; or to use limited local contexts,
at the risk of missing relevant information. In this work, we study a self-retrieval-augmented machine translation
framework (Self-RAMT), aimed at informing translation decisions with informative local and global contexts dynamically
extracted from the source and target texts. We examine the effectiveness of this method using three large language models,
considering three criteria for context selection. We carry out experiments on TED talks as well as parallel scientific
articles, considering three translation directions. Our results show that integrating distant contexts with Self-RAMT
improves translation quality as measured by reference-based scores and consistency metrics
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{peng-etal-2025-self,
title = "Self-Retrieval from Distant Contexts for Document-Level Machine Translation",
author = "Peng, Ziqian and Bawden, Rachel and Yvon, Fran{\c{c}}ois",
editor = "Haddow, Barry  and Kocmi, Tom and Koehn, Philipp and Monz, Christof",
booktitle = "Proceedings of the Tenth Conference on Machine Translation",
month = nov,
year = "2025",
address = "Suzhou, China",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2025.wmt-1.13/",
pages = "220--240",
ISBN = "979-8-89176-341-8",
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-05353046
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://www.uni-hildesheim.de/uccts2025/
">Is generative AI the solution to improve efficiency and objectivity in translation and post-editing quality assessment?</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Natalie K√ºbler, Alexandra Mestivier, Joachim Minder, Guillaume Wisniewski, Marie Bouchet, Maud B√©nard. Is generative AI the solution to improve efficiency and objectivity in translation and post-editing quality assessment? Proceedings of Using Corpora in Contrastive and Translation Studies. Hildesheim, Allemagne, Septembre 2025. 
</p>
    <small class="text-muted"></small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@InProceedings{kubler-etal-2025-generative,
 author="Natalie K√ºbler and Alexandra Mestivier and Joachim Minder and Guillaume Wisniewski and Marie Bouchet and Maud B√©nard",
 title={{Is generative AI the solution to improve efficiency and objectivity in translation and post-editing quality assessment?}},
 booktitle="Proceedings of Using Corpora in Contrastive and Translation Studies",
 year="2025",
 url="https://www.uni-hildesheim.de/uccts2025/",
 address = {Hildesheim, Allemagne}
 }
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://www.uni-hildesheim.de/uccts2025/
">Specialised Translation Annotation with LLMs: Error Identification and Categorisation Using a Customised Error Typology</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Joachim Minder, Guillaume Wisniewski, Natalie K√ºbler. Specialised Translation Annotation with LLMs: Error Identification and Categorisation Using a Customised Error Typology. Proceedings of Using Corpora in Contrastive and Translation Studies. Hildesheim, Allemagne, Septembre 2025. 
</p>
    <small class="text-muted"></small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@InProceedings{minder-etal-2025-specialized,
 author="Joachim Minder and Guillaume Wisniewski and Natalie K√ºbler",
 title={{Specialised Translation Annotation with LLMs: Error Identification and Categorisation Using a Customised Error Typology}},
 booktitle="Proceedings of Using Corpora in Contrastive and Translation Studies",
 year="2025",
 url="https://www.uni-hildesheim.de/uccts2025/",
 address = {Hildesheim, Allemagne}
 }
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="">On Assessing the Morphological and Multilingual Competence of LLMs
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Paul Lerner. On Assessing the Morphological and Multilingual Competence of LLMs. DAAD Postdoc-NeT-AI seminars, Munich, Heilbronn, and G√∂ttingen. 2025. 
</p>
    <small class="text-muted">Studying the morphological competence of LLMs allows us to measure their generalization ability.
Indeed, the lexicon is not a list of words that is known a priori and immutable. Modern models all rely on
BPE (Byte Pair Encoding) segmentation, which segments rare words into subwords by optimizing a data
compression criterion. Thus, models are theoretically capable of deriving or inflecting words in forms
absent from their training corpus, but the reality is more complex.
We‚Äôll study if LLMs are able to translate neologisms, i.e. new terms absent from their training data
(Lerner and Yvon, 2025a). We‚Äôll find that they can benefit from in-context learning with co-hyponyms and
terms sharing the same derivation paradigm. However, they are sensitive to the morphological similarity
between source and target terms. Their predictions are also impacted by subword tokenization, especially
for prefixed terms.
Because subwords are marked as initial- or intra-word, we find that LLMs perform poorly at handling
some types of affixations, which hinders their ability to generate novel (unobserved) word forms (Lerner
and Yvon, 2025b). The largest models trained on enough data can mitigate this tendency because their
initial- and intra-word embeddings are aligned; in-context learning also helps when all examples are
selected in a consistent way; but only morphological segmentation can achieve a near-perfect accuracy.
Apart from formal neologisms, formed by affixation, a common phenomena is borrowing, which is
linked to code-switching. Although all LLMs are now somewhat multilingual, the way they transfer
and share knowledge across languages is not yet well understood, and is likely very different from
human multilingualism. Indeed, LLMs are trained on a mix of multiple monolingual corpora, making
code-switching very surprising. We aim precisely at measuring this (Rado≈Ça, 2025). 
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://aclanthology.org/2025.mtsummit-1.15/ 
">Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Joachim Minder, Guillaume Wisniewski, Natalie K√ºbler. Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT. Proc. MT-Summit 2025. Gen√®ve. 
</p>
    <small class="text-muted">This study investigates the capabilities of large language models (LLMs), specifically ChatGPT, in annotating MT outputs based on an error typology. In contrast to previous work focusing mainly on general language, we explore ChatGPT's ability to identify and categorise errors in specialised translations. By testing two different prompts and based on a customised error typology, we compare ChatGPT annotations with human expert evaluations of translations produced by DeepL and ChatGPT itself. The results show that, for translations generated by DeepL, recall and precision are quite high. However, the degree of accuracy in error categorisation depends on the prompt's specific features and its level of detail, ChatGPT performing very well with a detailed prompt. When evaluating its own translations, ChatGPT achieves significantly poorer results, revealing limitations with self-assessment. These results highlight both the potential and the limitations of LLMs for translation evaluation, particularly in specialised domains. Our experiments pave the way for future research on open-source LLMs, which could produce annotations of comparable or even higher quality. In the future, we also aim to test the practical effectiveness of this automated evaluation in the context of translation training, particularly by optimising the process of human evaluation by teachers and by exploring the impact of annotations by LLMs on students' post-editing and translation learning.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@article{mindner-etal-2025-testing,
  TITLE = {{Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT}},
  AUTHOR = {Joachim Minder and Guillaume Wisniewski and Natalie K√ºbler},
  URL = {https://hal.science/hal-05144033v1 },
  PAGES = {4-16},
  BOOKTITLE = {{Proceedings of the Machine Translation Summit XX}},
  YEAR = {2025},
  MONTH = Jun,
  KEYWORDS = {Neural Machine Translation ; LLM-as-Judge ; Evaluation of Machine Translation},
  PDF = {https://aclanthology.org/2025.mtsummit-1.15.pdf},
  HAL_ID = {hal-05144033},
  HAL_VERSION = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-05144033v1
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://inria.hal.science/hal-04906015v1
">Investigating Length Issues in Document-level Machine Translation
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Ziqian Peng, Rachel Bawden, Fran√ßois Yvon. Investigating Length Issues in Document-level Machine Translation. Proc. MT-Summit 2025. Gen√®ve. 
</p>
    <small class="text-muted">Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters, and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@article{peng-etal-2025-investigating,
  TITLE = {{Investigating Length Issues in Document-level Machine Translation}},
  AUTHOR = {Peng, Ziqian and Bawden, Rachel and Yvon, Fran{\c c}ois},
  URL = {https://inria.hal.science/hal-04906015v1},
  PAGES = {4-16},
  BOOKTITLE = {{Proceedings of the Machine Translation Summit XX}},
  YEAR = {2025},
  MONTH = Jun,
  KEYWORDS = {Neural Machine Translation ; Context aware machine translation ; Document level machine translation},
  PDF = {https://aclanthology.org/2025.mtsummit-1.3.pdf},
  HAL_ID = {hal-04906015},
  HAL_VERSION = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://inria.hal.science/hal-04906015v1
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://changelinglab.github.io/
">Morphological Competence of LLMs: Applied to Translation of Scientific Neologisms</a></h4></div>
  <div class="card-body">
    <p class="font-italic">"Paul Lerner. Morphological Competence of LLMs: Applied to Translation of Scientific Neologisms. ChangeLing (CMU) seminars, online, https://changelinglab.github.io/. 2025."
</p>
    <small class="text-muted"></small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@InProceedings{Mortensen2025,
 author="Lerner, Paul",
 title={{Morphological Competence of LLMs:
 Applied to Translation of Scientific Neologisms}},
 booktitle="ChangeLing (CMU) seminars",
 year="2025",  
 url="https://changelinglab.github.io/",
 ADDRESS = {online}
 }
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://aclanthology.org/2025.coling-main.348
">Unlike ‚ÄúLikely‚Äù, ‚ÄúUnlike‚Äù is Unlikely: BPE-based Segmentation hurts Morphological Derivations in LLMs</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Paul Lerner and Fran√ßois Yvon. 2025. Unlike ‚ÄúLikely‚Äù, ‚ÄúUnlike‚Äù is Unlikely: BPE-based Segmentation hurts Morphological Derivations in LLMs. In Proceedings of the 31st International Conference on Computational Linguistics, pages 5181‚Äì5190, Abu Dhabi, UAE. Association for Computational Linguistics.
</p>
    <small class="text-muted">Large Language Models (LLMs) rely on subword vocabularies to process and generate text. However, because subwords are marked as initial- or intra-word, we find that LLMs perform poorly at handling some types of affixations, which hinders their ability to generate novel (unobserved) word forms. The largest models trained on enough data can mitigate this tendency because their initial- and intra-word embeddings are aligned; in-context learning also helps when all examples are selected in a consistent way; but only morphological segmentation can achieve a near-perfect accuracy. 
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{coling2025pvs,
title = {{Unlike ``Likely'', ``Unlike'' is Unlikely: BPE-based Segmentation hurts Morphological Derivations in LLMs}},
author={Lerner, Paul and Yvon, Fran√ßois},
booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
url={https://aclanthology.org/2025.coling-main.348/},
year = "2025",
publisher = "International Committee on Computational Linguistics"
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-04831106v1
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://aclanthology.org/2025.coling-main.63/
">Towards the Machine Translation of Scientific Neologisms</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Paul Lerner and Fran√ßois Yvon. 2025. Towards the Machine Translation of Scientific Neologisms. In Proceedings of the 31st International Conference on Computational Linguistics, pages 5181‚Äì5190, Abu Dhabi, UAE. Association for Computational Linguistics.
</p>
    <small class="text-muted">Scientific research continually discovers and invents new concepts, which are then referred to by new terms, neologisms, or neonyms in this context. As the vast majority of publications are written in English, disseminating this new knowledge to the general public often requires translating these terms. However, by definition, no parallel data exist to provide such translations. Therefore, we propose to leverage term definitions as a useful source of information for the translation process. As we discuss, Large Language Models are well suited for this task and can benefit from in-context learning with co-hyponyms and terms sharing the same derivation paradigm. These models, however, are sensitive to the superficial and morphological similarity between source and target terms. Their predictions are also impacted by subword tokenization, especially for prefixed terms. 
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{coling2025trad,
title = {{Towards the Machine Translation of Scientific Neologisms}},
author={Lerner, Paul and Yvon, Fran√ßois},
booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
year = "2025",
url={https://aclanthology.org/2025.coling-main.63/},
publisher = "International Committee on Computational Linguistics"
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-04835653v2
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://inria.hal.science/hal-04700009v1
">Survey of existing Human Metrics and Protocols for Document-Level Machine Translation</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Maud B√©nard, Natalie K√ºbler, Alexandra Mestivier, Joachim Minder et Lichao Zhu. √âtude des Protocoles d'√âvaluation Humaine pour la Traduction de Documents. Rapport D4-1.1, Projet ANR MaTOS. 2024, pp.84. 
</p>
    <small class="text-muted">This report provides an overview of the various protocols for evaluating the quality of human translation, machine translation and/or post-editing. After a brief summary of the automatic metrics developed in NLP, we focus on the evaluation protocols implemented by humans. Psychological approaches are distinguished from textual or discursive approaches. We discuss in more detail the description of textual approaches, i.e. mainly error typologies, in theoretical, professional and pedagogical contexts for evaluating the quality of human and machine translation and post-editing. Finally, we develop the new typology adapted to these three types of production, which is being implemented in the MaTOS project. The manual for this typology is presented in the appendix. 
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@techreport{benard:hal-04700009,
  TITLE = {{{\'E}tude des Protocoles d' {\'E}valuation Humaine pour la Traduction de Documents}},
  AUTHOR = {B{\'e}nard, Maud and K{\"u}bler, Natalie and Mestivier, Alexandra and Minder, Joachim and Zhu, Lichao},
  URL = {https://hal.science/hal-04700009},
  PAGES = {livrable D4-1.1, 83 pages},
  INSTITUTION = {{Projet ANR MaTOS}},
  YEAR = {2024},
  MONTH = Sep,
  KEYWORDS = {machine translation ; post-editing ; error typology ; error annotation ; {\'e}valuation humaine ; traduction automatique ; post-{\'e}dition ; typologie d'erreurs ; human evaluation},
  PDF = {https://hal.science/hal-04700009v1/file/d4-1.1.pdf},
  HAL_ID = {hal-04700009},
  HAL_VERSION = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://inria.hal.science/hal-04700009v1
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://inria.hal.science/hal-04652584
">Handling Very Long Contexts in Neural Machine Translation: a Survey
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Ziqian Peng, Rachel Bawden, Fran√ßois Yvon. Handling Very Long Contexts in Neural Machine Translation, a Survey. Livrable D3-2.1, Projet ANR MaTOS. 2024, pp.50.
</p>
    <small class="text-muted">Ce rapport √©tudie les m√©thodes visant √† int√©grer un contexte discursif √©tendu en traduction automatique (TA), en se focalisant sur les m√©thodes de traduction neuronales. Les syst√®mes de traduction automatique traduisent en g√©n√©ral chaque phrase ind√©pendemment de ses voisines, ce qui entra√Æne des erreurs syst√©matiques qui r√©sultent d'un contexte discursif trop √©troit. Diverses approches ont √©t√© propos√©es pour int√©grer le contexte au-del√† de la phrase courante, en s'appuyant  sur l'architecture transformeur, qui est l'architecture pr√©dominante en TA. R√©cemment, l'introduction de grands mod√®les de langue (LLM) a √©galement cr√©√© de nouvelles opportunit√©s pour traiter les d√©pendances √† longue port√©e, donnant lieu √† la formulation d'approches holistiques de la traduction, qui prennent en compte un contexte √©tendu. Nous discutons des d√©fis que pose la traduction de longs documents, avant de pr√©senter les m√©thodes propos√©es pour les architectures encodeurs-d√©codeurs et les approches √† base de LLM, avec un bref aper√ßu des impl√©mentations efficaces pour les transformeurs, qui subsubmment ces deux types de mod√®les. En compl√©ment, nous consid√©rons √©galement des strat√©gies d'extension de la f√™netre du contexte pour d'autres t√¢ches de TAL; nous avons √©galement list√© des corpus de documents parall√®les r√©cemment disponibles en source ouverte, pour une exploration future. Nous concluons par un r√©sum√© des travaux actuels et des principales directions de recherche.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@techreport{peng:hal-04652584,
  TITLE = {{Handling Very Long Contexts in Neural Machine Translation: a Survey}},
  AUTHOR = {Peng, Ziqian and Bawden, Rachel and Yvon, Fran{\c c}ois},
  URL = {https://inria.hal.science/hal-04652584},
  NUMBER = {Livrable D3-2.1},
  PAGES = {50},
  INSTITUTION = {{Projet ANR MaTOS}},
  YEAR = {2024},
  MONTH = Jun,
  KEYWORDS = {Neural Machine Translation ; Context aware machine translation ; Document level machine translation},
  PDF = {https://inria.hal.science/hal-04652584/file/report.pdf},
  HAL_ID = {hal-04652584},
  HAL_VERSION = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://inria.hal.science/hal-04652584
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://inria.hal.science/hal-04623021">Towards Machine Translation of Scientific Neologisms (In French)</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Paul Lerner, Fran√ßois Yvon. Vers la traduction automatique des n√©ologismes scientifiques. Proceedings of the 31st Conf√©rence sur le Traitement Automatique des Langues Naturelles, pages 245-261, Toulouse, France, ATALA.</p>
    <small class="text-muted">Scientific research continually discovers and invents new concepts, which are then referred to by new terms, neologisms, or neonyms in this context. As the vast majority of publications are written in English, disseminating this new knowledge in French often requires translating these terms, to avoid multiplying anglicisms that are less easily understood by the general public. We propose to explore this task using two thesauri, exploiting the definition of the term to translate it more accurately. To this end, we explore the capabilities of two large multilingual models, BLOOM and CroissantLLM, which can translate scientific terms to some extent. In particular, we show that they often use appropriate morphological procedures, but are limited by the segmentation into sub-lexical units. They are also biased by the frequency of term occurrences and surface similarities between English and French.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{lerner:hal-04623021,
    title = {{Vers la traduction automatique des n{\'e}ologismes scientifiques}},
    author = {Lerner, Paul and Yvon, Fran{\c c}ois},
    url = {https://inria.hal.science/hal-04623021},
    booktitle = {{Proceedings of the 31st Conf√©rence sur le Traitement Automatique des Langues Naturelles}},
    address = {Toulouse, France},
    editor = {BALAGUER, Mathieu and BENDAHMAN, Nihed and HO-DAC, Lydia-Mai and MAUCLAIR, Julie and MORENO, Jose G and PINQUIER, Julien},
    publisher = {{ATALA \& AFPC}},
    volume = {1 : articles longs et prises de position},
    pages = {245-261},
    year = {2024},
    month = Jul,
    keywords = {n{\'e}ologisme ; terminologie ; morphologie ; traduction automatique},
    pdf = {https://inria.hal.science/hal-04623021/file/9096.pdf},
    hal_id = {hal-04623021},
    HAL_VERSION = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://inria.hal.science/hal-04623021
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://hal.science/hal-04623006">Document Level Machine Translation: does length matter? (In French) 
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Ziqian Peng, Rachel Bawden and Fran√ßois Yvon (2024). Document Level Machine Translation: does length matter?. Proceedings of the 31st Conf√©rence sur le Traitement Automatique des Langues Naturelles, pages 2-21, Toulouse, France, ATALA.
</p>
    <small class="text-muted">Today‚Äôs machine translation architectures can process long segments and go beyond the translation of isolated sentences, opening up the possibility of translating full documents. To achieve this goal, it is necessary to overcome several difficulties related to the length of source documents. In this work, we discuss document-level machine translation from an evaluation perspective, trying to answer a simple question: how can we measure whether translation performance degrades with document length? Our analysis, which compares encoder-decoder systems and a large language model using multiple metrics on a scientific document translation task, suggests that translating long documents holistically remains a challenging problem.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{peng:hal-04623006,
 TITLE = {{{\`A} propos des difficult{\'e}s √† traduire automatiquement de longs documents}},
 AUTHOR = {Peng, Ziqian and Bawden, Rachel and Yvon, Fran{\c c}ois},
 URL = {https://inria.hal.science/hal-04623006},
   booktitle = {{Proceedings of the 31st Conf√©rence sur le Traitement Automatique des Langues Naturelles}},
   address = {Toulouse, France},
   editor = {BALAGUER, Mathieu and BENDAHMAN, Nihed and HO-DAC, Lydia-Mai and MAUCLAIR, Julie and MORENO, Jose G and PINQUIER, Julien},
   publisher = {{ATALA \& AFPC}},
   volume = {1 : articles longs et prises de position},
   pages = {2-21},
   year = {2024},
   month = Jul,
   keywords = {Traduction Automatique ; {\'E}valuation de la traduction ; Traitement de documents},
   pdf = {https://inria.hal.science/hal-04623006/file/0157.pdf},
   hal_ID = {hal-04623006},
   hal_version = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-04623006" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://anr.hal.science/hal-04435371v1">Les mod√®les Bloom pour le traitement automatique de la langue fran√ßaise</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Rachel Bawden, Hatim Bourfoune, Bertrand Cabot, Nathan Cassereau, Pierre Cornette, Marco Naguib, Aur√©lie N√©v√©ol and Fran√ßois Yvon. Les mod√®les Bloom pour le traitement automatique de la langue fran√ßaise. 2024. Technical report.</p>
    <small class="text-muted">The development of very large language models, capable of performing a large range of automatic language processing tasks, simultaneously requires to develop the infrastructure needed to evaluate these models, ideally covering as many tasks as possible. Numerous benchmarks have already been compiled for the English language, making it possible to evaluate these large models from multiple angles. Several multilingual test sets are also available, with a much lesser coverage, which are used to measure the ability of these models to handle multiple languages. In this paper, we present our efforts to assemble a multi-task evaluation set for French, which is then used to evaluate models from the BLOOM family. Our results confirm and complement the main evaluation results for BLOOM in English; they allow us to conclude that the performances obtained in French and English are very similar and even better when the prompts used at inference are written in the same language as the texts to analyze.</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://anr.hal.science/hal-04435371v1" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://eamt2024.github.io/proceedings/vol1.pdf">Translate your Own: a Post-Editing Experiment in the NLP domain
</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Rachel Bawden, Ziqian Peng, Maud B√©nard, Eric Villemonte de La Clergerie, Rapha√´l Esamotunu, Mathilde Huguin, Natalie K√ºbler, Alexandra Mestivier, Mona Michelot, Laurent Romary, Lichao Zhu and Fran√ßois Yvon (2024). Translate your Own: a Post-Editing Experiment in the NLP domain. In Proceedings of the 25th Annual Conference of the European Association for Machine Translation, pages 431‚Äì443, Sheffield, UK, European Association for Machine Translation.
</p>
    <small class="text-muted">The improvements in neural machine translation make translation and post- editing pipelines ever more effective for a wider range of applications. In this paper, we evaluate the effectiveness of such a pipeline for the translation of scientific documents (limited here to article abstracts). Using a dedicated interface, we collect, then analyse the post-edits of approximately 350 abstracts (English‚ÜíFrench) in the Natural Language Processing domain for two groups of post-editors: domain experts (academics encouraged to post-edit their own articles) on the one hand and trained translators on the other. Our results confirm that such pipelines can be effective, at least for high-resource language pairs. They also highlight the difference in the post-editing strategy of the two subgroups. Finally, they suggest that working on term translation is the most pressing issue to improve fully automatic translations, but that in a post-editing setup, other error types can be equally annoying for post-editors.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{benard-etal-2023-matos,
      title = {{Translate your Own: a Post-Editing Experiment in the NLP domain]},
      author = {Rachel Bawden and Ziqian Peng and Maud B√©nard and Villemonte de La Clergerie, Eric and Rapha√´l Esamotunu and Mathilde Huguin and Natalie K√ºbler and Alexandra Mestivier and Mona Michelot and Laurent Romary and Lichao Zhu and Fran√ßois Yvon},
      booktitle = {Proceedings of the 25th Annual Conference of the European Association for Machine Translation},
      year = {2024},
      address = {Sheffield, UK},
      publisher = {European Association for Machine Translation},
      url = { https://anr.hal.science/hal-04573922v1},
      pages = {431--433},
      abstract = {The improvements in neural machine translation make translation and post- editing pipelines ever more effective for a wider range of applications. In this paper, we evaluate the effectiveness of such a pipeline for the translation of scientific documents (limited here to article abstracts). Using a dedicated interface, we collect, then analyse the post-edits of approximately 350 abstracts (English‚ÜíFrench) in the Natural Language Processing domain for two groups of post-editors: domain experts (academics encouraged to post-edit their own articles) on the one hand and trained translators on the other. Our results confirm that such pipelines can be effective, at least for high-resource language pairs. They also highlight the difference in the post-editing strategy of the two subgroups. Finally, they suggest that working on term translation is the most pressing issue to improve fully automatic translations, but that in a post-editing setup, other error types can be equally annoying for post-editors.},
      language = {French},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://anr.hal.science/hal-04573922v1" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://doi.org/10.1016/j.csl.2024.101623
">Translating scientific abstracts in the bio-medical domain with structure-aware models</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Sadaf Abdul Rauf, Fran√ßois Yvon (2024). Translating scientific abstracts in the bio-medical domain with structure-aware models. Computer Speech & Language, vol. 87.</p>
    <small class="text-muted">Machine Translation (MT) technologies have improved in many ways and generate usable outputs for a growing number of domains and language pairs. Yet, most sentence based MT systems struggle with contextual dependencies, processing small chunks of texts, typically sentences, in isolation from their textual context. This is likely to cause systematic errors or inconsistencies when processing long documents. While various attempts are made to handle extended contexts in translation, the relevance of these contextual cues, especially those related to the structural organization, and the extent to which they affect translation quality remains an under explored area. In this work, we explore ways to take these structural aspects into account, by integrating document structure as an extra conditioning context. Our experiments on biomedical abstracts, which are usually structured in a rigid way, suggest that this type of structural information can be useful for MT and document structure prediction. We also present in detail the impact of structural information on MT output and assess the degree to which structural information can be learned from the data.
</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@article{abdulrauf-yvon-2024-translating,
    title = {Translating scientific abstracts in the bio-medical domain with structure-aware models},
    volume = {87},
    issn = {0885-2308},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230824000068},
    doi = {https://doi.org/10.1016/j.csl.2024.101623},
    journal = {Computer Speech \& Language},
    author = {Abdul Rauf, Sadaf and Yvon, Fran√ßois},
    year = {2024},
    keywords = {Bio-medical natural language processing, Document-level machine translation, Neural machine translation},
    pages = {101623},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-04476788
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://cnrs.hal.science/ISIR/hal-04258660v1
">Document-level Machine Translation for scientific texts</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Ziqian Peng (2023). Document-level Machine Translation for scientific texts. M√©moire de Master, Universit√© Paris-Saclay.</p>
    <small class="text-muted">While neural machine translation has seen significant progress during recent years at sentencelevel, translating full documents remains a challenge to efficiently incorporate document-level context. Various approaches have been proposed, but most of them consider only one to three previous source and/or target sentences as the context. This is not sufficient to faithfully translate some language phenomena, like lexical consistency and document coherence, especially in some scientific texts. In this work, we conducted experiments to include full contextual context and investigate the impact of all the past / future sentences on the source side with a context ablation study, on some abstracts from scientific publications. Our results show that future context is more influential than the past source context, and in our experiments, the Transformer architecture performs much better to translate the beginning of a long document than the end.</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@techreport{peng:hal-04258660,
     title = {{Document-level Machine Translation For Scientific Texts}},
     author = {Peng, Ziqian},
     url = {https://hal.science/hal-04258660},
     institution = {{ISIR, Universit{\'e} Pierre et Marie Curie UMR CNRS 7222}},
     year = {2023},
     month = Sep,
     pdf = {https://hal.science/hal-04258660v1/file/main.pdf},
     hal_id = {hal-04258660},
     hal_version = {v1},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://cnrs.hal.science/ISIR/hal-04258660v1
" style="text-decoration:none" class="text-white">HAL </a> </button>

  </div>
</div>


<div class="card bg-white mb-3">
  <div class="card-header mb-0"><h5 class="card-title mb-0"><a href="https://aclanthology.org/2023.jeptalnrecital-arts.2/
">MaTOS Traduction automatique pour la science ouverte</a></h4></div>
  <div class="card-body">
    <p class="font-italic">Maud B√©nard, Alexandra Mestivier, Natalie K√ºbler, Lichao Zhu, Rachel Bawden, √âric De La Clergerie, Laurent Romary, Mathilde Huguin, Jean-Fran√ßois Nomin√©, Ziqian Peng, Fran√ßois Yvon (2023). MaTOS Traduction automatique pour la science ouverte. Actes de l'Atelier sur l'Analyse et la Recherche de Textes Scientifiques, CORIA-TALN 2023. 5 juin 2023 Paris (France).</p>
    <small class="text-muted">This contribution presents the MaTOS (Machine Translation for Open Science) project, which aims to develop new methods for the complete machine translation (MT) of scientific documents between English and French, as well as automatic metrics to evaluate the translation quality. To this end, MaTOS is interested in (a) the collection of open resources for specialised MT ; (b) the description of textual coherence markers for scientific articles ; (c) the development of new multilingual processing methods for documents ; and (d) metrics to measure progress in document-level machine translation.</small>
    

<br>
 <button type="button" class="btn btn-secondary btn-sm btn-pub-copy
 mt-3 mb-3 copyable" name="@inproceedings{benard-etal-2023-matos,
     title = {{M}a{TOS}: Traduction automatique pour la science ouverte},
     author = {B{\'e}nard, Maud and Mestivier, Alexandra and Kubler, Natalie and Zhu, Lichao and Bawden, Rachel and De La Clergerie, Eric and Romary, Laurent and Huguin, Mathilde  and Nomin{\'e}, Jean-Fran{\c{c}}ois  and Peng, Ziqian and Yvon, Fran{\c{c}}ois},
     editor = {Boudin, Florian and Daille, B{\'e}atrice  and Dufour, Richard  and El, Oumaima and Houbre, Ma√´ll and Jourdan, L{\'e}ane and Kooli, Nihel},
     booktitle = {Actes de CORIA-TALN 2023. Actes de l'atelier ``Analyse et Recherche de Textes Scientifiques'' (ARTS)@TALN 2023},
     month = {6},
     year = {2023},
     address = {Paris, France},
     publisher = {ATALA},
     url = {https://aclanthology.org/2023.jeptalnrecital-arts.2},
     pages = {8--15},
     abstract = {This contribution presents the MaTOS (Machine Translation for Open Science) project, which aims to develop new methods for the complete machine translation (MT) of scientific documents between English and French, as well as automatic metrics to evaluate the translation quality. To this end, MaTOS is interested in (a) the collection of open resources for specialised MT ; (b) the description of textual coherence markers for scientific articles ; (c) the development of new multilingual processing methods for documents ; and (d) metrics to measure progress in document-level machine translation.},
     language = {French},
}
" lang="en" data-original-title="" title="" aria-describedby="tooltip370027"><i class="far fa-clipboard pr-2"></i>BibTex</button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1"
    lang="en"><i class="far fa-file-pdf pr-2"></i><a href="https://hal.science/hal-04131594
" style="text-decoration:none" class="text-white">HAL </a> </button>

    <button type="button" class="btn btn-secondary btn-sm mt-1 mb-1" lang="en"><a href=""
    style="text-decoration:none" class="text-white"><img src="https://aclanthology.org/images/acl-logo.svg" style="height:12px;"> ACL Anthology </a> </button>
  </div>
</div>

    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <!-- <ul class="col-sm-6 list-inline"> -->
    <!-- <li class="list-inline-item"><a href="https://anr-matos.github.io/en/archives.html">Archives</a></li> -->
    <!-- <li class="list-inline-item"><a href="https://anr-matos.github.io/en/pages/categories">Categories</a></li> -->
  <!-- </ul> -->

  <ul id="logos" class="list-group list-group-horizontal container-fluid borderless">
    <li class="list-group-item p-3"><img src="https://anr-matos.github.io/en/static/images/CNRS.png"></li>
    <li class="list-group-item p-3"><img src="https://anr-matos.github.io/en/static/images/Logo-sorbonne.jpg"></li>
    <li class="list-group-item p-3"><img src="https://anr-matos.github.io/en/static/images/inria.png"></li>
    <li class="list-group-item p-3"><img src="https://anr-matos.github.io/en/static/images/Universite_Paris-Cite-logo.jpeg"></li>
    <li class="list-group-item p-3 ml-auto"><img src="https://anr-matos.github.io/en/static/images/Logo-ANR.jpg"></li>
  </ul>
  
  <p class="col-sm-6 text-sm-left text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>



    <!-- jQuery library -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.slim.min.js"></script>
    <!-- Popper JS -->
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
    <!-- Latest compiled JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://anr-matos.github.io/en/theme/js/copy_to_clipboard.js"></script>
    
</body>

</html>